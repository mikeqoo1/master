# Efficient Processing of Deep Neural Networks: A Tutorial and Survey çš„é–±è®€

## å‰è¨€

```txt
Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems

æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰ç›®å‰å»£æ³›æ‡‰ç”¨æ–¼è¨±å¤šäººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ‡‰ç”¨ï¼ŒåŒ…æ‹¬è¨ˆç®—æ©Ÿè¦–è¦ºã€èªéŸ³è­˜åˆ¥å’Œæ©Ÿå™¨äººæŠ€è¡“ã€‚é›–ç„¶DNNåœ¨è¨±å¤šAIä»»å‹™ä¸Šæä¾›äº†æœ€å…ˆé€²çš„æº–ç¢ºæ€§ï¼Œä½†é€™æ˜¯ä»¥é«˜è¨ˆç®—è¤‡é›œæ€§ç‚ºä»£åƒ¹çš„ã€‚å› æ­¤ï¼Œå¯¦ç¾DNNé«˜æ•ˆè™•ç†çš„æŠ€è¡“å°æ–¼æé«˜èƒ½æºæ•ˆç‡å’Œååé‡è€Œä¸çŠ§ç‰²æ‡‰ç”¨æº–ç¢ºæ€§æˆ–å¢åŠ ç¡¬ä»¶æˆæœ¬è‡³é—œé‡è¦ï¼Œé€™å°æ–¼DNNåœ¨AIç³»çµ±ä¸­çš„å»£æ³›éƒ¨ç½²è‡³é—œé‡è¦ã€‚

This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and 
practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number 
of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: 
understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs 
between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.

é€™ç¯‡æ–‡ç« æ—¨åœ¨æä¾›é—œæ–¼å¯¦ç¾é«˜æ•ˆæ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰è™•ç†çš„æœ€æ–°é€²å±•çš„ç¶œåˆæ•™ç¨‹å’Œèª¿æŸ¥ã€‚å…·é«”ä¾†èªªï¼Œå®ƒå°‡æ¦‚è¿°DNNï¼Œè¨è«–æ”¯æŒDNNçš„å„ç¨®ç¡¬ä»¶å¹³å°å’Œæ¶æ§‹ï¼Œä¸¦é‡é»ä»‹ç´¹é€šéç¡¬ä»¶è¨­è¨ˆè®Šæ›´æˆ–ç¡¬ä»¶è¨­è¨ˆèˆ‡DNNç®—æ³•è®Šæ›´ç›¸çµåˆä¾†é™ä½DNNè¨ˆç®—æˆæœ¬çš„é—œéµè¶¨å‹¢ã€‚å®ƒé‚„å°‡ç¸½çµå„ç¨®é–‹ç™¼è³‡æºï¼Œä½¿ç ”ç©¶äººå“¡å’Œå¾æ¥­è€…èƒ½å¤ å¿«é€Ÿå…¥é–€é€™ä¸€é ˜åŸŸï¼Œä¸¦å¼·èª¿æ‡‰ç”¨æ–¼è©•ä¼°å­¸è¡“ç•Œå’Œå·¥æ¥­ç•Œæå‡ºçš„å¿«é€Ÿå¢é•·çš„DNNç¡¬ä»¶è¨­è¨ˆï¼ˆå¯é¸åŒ…æ‹¬ç®—æ³•å”åŒè¨­è¨ˆï¼‰çš„é‡è¦åŸºæº–æ¸¬è©¦æŒ‡æ¨™å’Œè¨­è¨ˆè€ƒæ…®å› ç´ ã€‚è®€è€…å°‡å¾é€™ç¯‡æ–‡ç« ä¸­äº†è§£ä»¥ä¸‹æ¦‚å¿µï¼š

- ç†è§£æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰çš„é—œéµè¨­è¨ˆè€ƒæ…®å› ç´ ï¼›

- èƒ½å¤ ä½¿ç”¨åŸºæº–æ¸¬è©¦å’Œæ¯”è¼ƒæŒ‡æ¨™è©•ä¼°ä¸åŒçš„DNNç¡¬ä»¶å¯¦ç¾ï¼›

- ç†è§£ä¸åŒç¡¬ä»¶æ¶æ§‹å’Œå¹³å°ä¹‹é–“çš„æ¬Šè¡¡ï¼›

- èƒ½å¤ è©•ä¼°å„ç¨®DNNè¨­è¨ˆæŠ€è¡“çš„å¯¦ç”¨æ€§ï¼Œä»¥å¯¦ç¾é«˜æ•ˆè™•ç†ï¼›

- äº†è§£æœ€è¿‘çš„å¯¦æ–½è¶¨å‹¢å’Œæ©Ÿæœƒã€‚

KEYWORDS
ASIC; æ‡‰ç”¨å°ˆç”¨é›†æˆé›»è·¯
computer architecture; è¨ˆç®—æ©Ÿæ¶æ§‹
convolutional neural networks; å·ç©ç¥ç¶“ç¶²çµ¡
dataflow processing; æ•¸æ“šæµè™•ç†
deep learning; æ·±åº¦å­¸ç¿’
deep neural networks; æ·±åº¦ç¥ç¶“ç¶²çµ¡
energy-efficient accelerators; èƒ½æ•ˆåŠ é€Ÿå™¨
low power; ä½åŠŸè€—
machine learning; æ©Ÿå™¨å­¸ç¿’
spatial architectures; ç©ºé–“æ¶æ§‹
VLSI; è¶…å¤§è¦æ¨¡é›†æˆé›»è·¯
```

## ç¬¬ä¸€ç¯€

```txt
Deep neural networks (DNNs) are currently the foundation for many modern artificial intelligence (AI) applications [1]. Since the breakthrough application of DNNs to speech recognition [2] and image recognition [3], the number of applications that use DNNs has exploded. These DNNs are employed in a myriad of applications from selfdriving cars [4], to detecting cancer [5] to playing complex games [6]. In many of these domains, DNNs are now able to exceed human accuracy. The superior performance of DNNs comes from its ability to extract high-level features from raw sensory data after using statistical learning over a large amount of data to obtain an effective representation of an input space. This is different from earlier approaches that use hand-crafted features or rules designed by experts. The superior accuracy of DNNs, however, comes at the cost of high computational complexity. While general-purpose compute engines, especially graphics processing units (GPUs), have been the mainstay for much DNN processing, increasingly there is interest in providing more specialized acceleration of the DNN computation. This article aims to provide an overview of DNNs, the various tools for understanding their behavior, and the techniques being explored to efficiently accelerate their computation.

æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰ç›®å‰æ˜¯è¨±å¤šç¾ä»£äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ‡‰ç”¨çš„åŸºç¤[1]ã€‚è‡ªå¾DNNåœ¨èªéŸ³è­˜åˆ¥[2]å’Œåœ–åƒè­˜åˆ¥[3]æ–¹é¢å–å¾—çªç ´æ€§æ‡‰ç”¨ä»¥ä¾†ï¼Œä½¿ç”¨DNNçš„æ‡‰ç”¨æ•¸é‡æ¿€å¢ã€‚é€™äº›DNNè¢«æ‡‰ç”¨æ–¼å¾è‡ªé§•è»Š[4]ã€ç™Œç—‡æª¢æ¸¬[5]åˆ°ç©è¤‡é›œéŠæˆ²[6]çš„å„ç¨®æ‡‰ç”¨ä¸­ã€‚åœ¨è¨±å¤šé€™äº›é ˜åŸŸï¼ŒDNNç¾åœ¨èƒ½å¤ è¶…è¶Šäººé¡çš„æº–ç¢ºæ€§ã€‚DNNçš„å“è¶Šæ€§èƒ½ä¾†è‡ªæ–¼å…¶èƒ½å¤ å¾åŸå§‹æ„Ÿå®˜æ•¸æ“šä¸­æå–é«˜ç´šç‰¹å¾µï¼Œä¸¦é€šéå°å¤§é‡æ•¸æ“šé€²è¡Œçµ±è¨ˆå­¸ç¿’ä¾†ç²å¾—æœ‰æ•ˆçš„è¼¸å…¥ç©ºé–“è¡¨ç¤ºã€‚é€™èˆ‡æ—©æœŸä½¿ç”¨å°ˆå®¶è¨­è¨ˆçš„æ‰‹å·¥ç‰¹å¾µæˆ–è¦å‰‡çš„æ–¹æ³•ä¸åŒã€‚
ç„¶è€Œï¼ŒDNNçš„å“è¶Šæº–ç¢ºæ€§æ˜¯ä»¥é«˜è¨ˆç®—è¤‡é›œæ€§ç‚ºä»£åƒ¹çš„ã€‚é›–ç„¶é€šç”¨è¨ˆç®—å¼•æ“ï¼Œç‰¹åˆ¥æ˜¯åœ–å½¢è™•ç†å–®å…ƒï¼ˆGPUï¼‰ï¼Œä¸€ç›´æ˜¯è¨±å¤šDNNè™•ç†çš„ä¸»åŠ›ï¼Œä½†è¶Šä¾†è¶Šå¤šçš„äººå°æä¾›æ›´å°ˆé–€çš„DNNè¨ˆç®—åŠ é€Ÿæ„Ÿèˆˆè¶£ã€‚é€™ç¯‡æ–‡ç« æ—¨åœ¨æ¦‚è¿°DNNã€ç†è§£å…¶è¡Œç‚ºçš„å„ç¨®å·¥å…·ä»¥åŠæ¢ç´¢é«˜æ•ˆåŠ é€Ÿå…¶è¨ˆç®—çš„æŠ€è¡“ã€‚

This paper is organized as follows.
â€¢â€‚ Section II provides background on the context of why
DNNs are important, their history and applications.
â€¢â€‚ Section III gives an overview of the basic components
of DNNs and popular DNN models currently in use.
â€¢â€‚ Section IV describes the various resources used for
DNN research and development.
â€¢â€‚ Section V describes the various hardware platform
used to process DNNs and the various optimizations 
used to improve throughput and energy efficiency 
without impacting application accuracy (i.e., produce 
bitwise identical results).
â€¢â€‚ Section VI discusses how mixed-signal circuits and new
memory technologies can be used for near-data processing to address the expensive data movement that dominates throughput and energy consumption of DNNs.
â€¢â€‚ Section VII describes various joint algorithm and hardware optimizations that can be performed on DNNs t
improve both throughput and energy efficiency while 
trying to minimize impact on accuracy.
â€¢â€‚ Section VIII describes the key metrics that should be
considered when comparing various DNN designs.

é€™ç¯‡æ–‡ç« çš„çµ„ç¹”å¦‚ä¸‹ï¼š

- ç¬¬äºŒç¯€ æä¾›äº†ç‚ºä»€éº¼æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰é‡è¦çš„èƒŒæ™¯ã€æ­·å²å’Œæ‡‰ç”¨ã€‚

- ç¬¬ä¸‰ç¯€ æ¦‚è¿°äº†DNNçš„åŸºæœ¬çµ„æˆéƒ¨åˆ†å’Œç›®å‰ä½¿ç”¨çš„æµè¡ŒDNNæ¨¡å‹ã€‚

- ç¬¬å››ç¯€ æè¿°äº†ç”¨æ–¼DNNç ”ç©¶å’Œé–‹ç™¼çš„å„ç¨®è³‡æºã€‚

- ç¬¬äº”ç¯€ æè¿°äº†ç”¨æ–¼è™•ç†DNNçš„å„ç¨®ç¡¬ä»¶å¹³å°å’Œå„ç¨®å„ªåŒ–æŠ€è¡“ï¼Œä»¥åœ¨ä¸å½±éŸ¿æ‡‰ç”¨æº–ç¢ºæ€§çš„æƒ…æ³ä¸‹æé«˜ååé‡å’Œèƒ½æºæ•ˆç‡ï¼ˆå³ç”¢ç”Ÿä½å…ƒç›¸åŒçš„çµæœï¼‰ã€‚

- ç¬¬å…­ç¯€ è¨è«–äº†å¦‚ä½•ä½¿ç”¨æ··åˆä¿¡è™Ÿé›»è·¯å’Œæ–°å‹å­˜å„²æŠ€è¡“é€²è¡Œè¿‘æ•¸æ“šè™•ç†ï¼Œä»¥è§£æ±ºä¸»å°DNNååé‡å’Œèƒ½è€—çš„æ˜‚è²´æ•¸æ“šç§»å‹•å•é¡Œã€‚

- ç¬¬ä¸ƒç¯€ æè¿°äº†å¯ä»¥åœ¨DNNä¸ŠåŸ·è¡Œçš„å„ç¨®è¯åˆç®—æ³•å’Œç¡¬ä»¶å„ªåŒ–ï¼Œä»¥åœ¨ç›¡é‡æ¸›å°‘å°æº–ç¢ºæ€§å½±éŸ¿çš„æƒ…æ³ä¸‹æé«˜ååé‡å’Œèƒ½æºæ•ˆç‡ã€‚

- ç¬¬å…«ç¯€ æè¿°äº†åœ¨æ¯”è¼ƒå„ç¨®DNNè¨­è¨ˆæ™‚æ‡‰è€ƒæ…®çš„é—œéµæŒ‡æ¨™ã€‚
```

## ç¬¬äºŒç¯€ ä»‹ç´¹ DNN çš„èƒŒæ™¯

A. AI è·Ÿ æ·±åº¦ç¥ç¶“ç¶²çµ¡(DNNs)

```txt
å°±æ˜¯ç§‘å­¸å®¶å…ˆå˜—è©¦ç”¨äººé¡çš„å¤§è…¦ç•¶ä½œæ©Ÿå™¨å­¸ç¿’çš„åƒè€ƒä¾†æº

æŠŠæˆ‘å€‘çš„å¤§è…¦çš„é‹ä½œæ¦‚å¿µæ‹¿å‡ºä¾† æˆ‘å€‘çš„å¤§è…¦ä¸»è¦å·¥ä½œåŸç†å¦‚ä¸‹

å¤§è…¦çš„ä¸»è¦è¨ˆç®—å–®å…ƒæ˜¯ç¥ç¶“å…ƒï¼ˆneuronï¼‰ã€‚åœ¨ä¸€å€‹å¹³å‡äººé¡å¤§è…¦ä¸­ï¼Œç´„æœ‰ 860 å„„å€‹ç¥ç¶“å…ƒã€‚ç¥ç¶“å…ƒå½¼æ­¤é€šéæ¨¹çªï¼ˆdendritesï¼‰å’Œè»¸çªï¼ˆaxonï¼‰ç›¸é€£ï¼Œå‰è€…ç‚ºè¼¸å…¥å…ƒç´ ï¼Œå¾Œè€…ç‚ºè¼¸å‡ºå…ƒç´ ï¼Œå¦‚åœ– 2 æ‰€ç¤ºã€‚ç¥ç¶“å…ƒé€šéæ¨¹çªæ¥æ”¶è¼¸å…¥ä¿¡è™Ÿï¼Œå°é€™äº›ä¿¡è™Ÿé€²è¡Œè¨ˆç®—ï¼Œä¸¦åœ¨è»¸çªä¸Šç”Ÿæˆè¼¸å‡ºä¿¡è™Ÿã€‚é€™äº›è¼¸å…¥å’Œè¼¸å‡ºä¿¡è™Ÿè¢«ç¨±ç‚ºæ¿€æ´»ï¼ˆactivationsï¼‰ã€‚ä¸€å€‹ç¥ç¶“å…ƒçš„è»¸çªåˆ†æ”¯èˆ‡è¨±å¤šå…¶ä»–ç¥ç¶“å…ƒçš„æ¨¹çªç›¸é€£ã€‚è»¸çªåˆ†æ”¯èˆ‡æ¨¹çªä¹‹é–“çš„é€£æ¥è¢«ç¨±ç‚ºçªè§¸ï¼ˆsynapseï¼‰ã€‚

å»¶ä¼¸ä¸‹ä¾†åˆ°æ©Ÿå™¨å­¸ç¿’ä¸Š

çªè§¸çš„ä¸€å€‹é—œéµç‰¹æ€§æ˜¯å®ƒèƒ½å¤ å°ç©¿éå®ƒçš„ä¿¡è™Ÿï¼ˆğ‘¥ğ‘–ï¼‰é€²è¡Œç¸®æ”¾ã€‚é€™å€‹ç¸®æ”¾å› å­è¢«ç¨±ç‚ºæ¬Šé‡ï¼ˆğ‘¤ğ‘–â€‹ï¼‰ã€‚
æ“šæ¨æ¸¬ï¼Œå¤§è…¦çš„å­¸ç¿’æ–¹å¼æ˜¯é€šéèª¿æ•´èˆ‡çªè§¸ç›¸é—œçš„æ¬Šé‡å¯¦ç¾çš„ã€‚å› æ­¤ï¼Œä¸åŒçš„æ¬Šé‡æœƒå°ç›¸åŒçš„è¼¸å…¥ç”¢ç”Ÿä¸åŒçš„åæ‡‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå­¸ç¿’æ˜¯æŒ‡æ¬Šé‡å› å­¸ç¿’åˆºæ¿€è€Œé€²è¡Œçš„èª¿æ•´ï¼Œè€Œå¤§è…¦çš„çµ„ç¹”çµæ§‹ï¼ˆå¯ä»¥è¦–ç‚ºé¡ä¼¼æ–¼ç¨‹åºçš„éƒ¨åˆ†ï¼‰ä¸¦ä¸æ”¹è®Šã€‚é€™ä¸€ç‰¹æ€§ä½¿å¤§è…¦æˆç‚ºæ©Ÿå™¨å­¸ç¿’é¢¨æ ¼ç®—æ³•çš„æ¥µä½³éˆæ„Ÿä¾†æºã€‚

åœ¨å—å¤§è…¦å•Ÿç™¼çš„è¨ˆç®—ç¯„ç–‡å…§ï¼Œæœ‰ä¸€å€‹å­é ˜åŸŸç¨±ç‚ºã€Œè„ˆè¡è¨ˆç®—ã€ï¼ˆspiking computingï¼‰ã€‚è©²é ˜åŸŸçš„éˆæ„Ÿä¾†è‡ªæ–¼é€™æ¨£çš„äº‹å¯¦ï¼šåœ¨æ¨¹çªå’Œè»¸çªä¹‹é–“çš„é€šä¿¡æ˜¯è„ˆè¡å½¢å¼çš„ä¿¡è™Ÿï¼Œä¸”ä¿¡æ¯çš„å‚³éä¸¦ä¸åƒ…åƒ…å–æ±ºæ–¼è„ˆè¡çš„å¹…åº¦ï¼Œè€Œæ˜¯èˆ‡è„ˆè¡åˆ°é”çš„æ™‚é–“æœ‰é—œã€‚ç¥ç¶“å…ƒä¸­çš„è¨ˆç®—ä¸¦ä¸åƒ…ä¾è³´æ–¼å–®ä¸€å€¼ï¼Œè€Œæ˜¯ä¾è³´æ–¼è„ˆè¡çš„å¯¬åº¦åŠä¸åŒè„ˆè¡ä¹‹é–“çš„æ™‚é–“é—œä¿‚ã€‚ä¸€å€‹å—è„ˆè¡å¤§è…¦ç‰¹æ€§å•Ÿç™¼çš„å…¸å‹é …ç›®æ˜¯ IBM çš„ TrueNorthã€‚

èˆ‡è„ˆè¡è¨ˆç®—ç›¸å°ï¼Œå¦ä¸€å€‹å—å¤§è…¦å•Ÿç™¼çš„è¨ˆç®—å­é ˜åŸŸæ˜¯ã€Œç¥ç¶“ç¶²çµ¡ã€ï¼ˆneural networksï¼‰ï¼Œè€Œé€™æ­£æ˜¯æœ¬æ–‡çš„é‡é»æ‰€åœ¨ã€‚
```

B. ç¥ç¶“ç¶²çµ¡èˆ‡æ·±åº¦ç¥ç¶“ç¶²çµ¡ (DNNs)

```txt
ç¥ç¶“ç¶²çµ¡çš„éˆæ„Ÿä¾†è‡ªæ–¼é€™æ¨£çš„æ¦‚å¿µï¼šç¥ç¶“å…ƒçš„è¨ˆç®—æ¶‰åŠè¼¸å…¥å€¼çš„åŠ æ¬Šå’Œã€‚é€™äº›åŠ æ¬Šå’Œå°æ‡‰æ–¼çªè§¸é€²è¡Œçš„ä¿¡è™Ÿç¸®æ”¾ä»¥åŠç¥ç¶“å…ƒå…§éƒ¨å°é€™äº›å€¼çš„çµåˆã€‚æ­¤å¤–ï¼Œç¥ç¶“å…ƒçš„è¼¸å‡ºä¸åƒ…åƒ…æ˜¯åŠ æ¬Šå’Œï¼Œå› ç‚ºå¦‚æœåƒ…æ˜¯é€™æ¨£ï¼Œå‰‡ä¸€ç³»åˆ—ç¥ç¶“å…ƒçš„è¨ˆç®—å°‡åƒ…åƒ…æ˜¯ç°¡å–®çš„ç·šæ€§ä»£æ•¸æ“ä½œã€‚ç›¸åï¼Œåœ¨ç¥ç¶“å…ƒå…§éƒ¨å­˜åœ¨ä¸€å€‹å°çµ„åˆè¼¸å…¥é€²è¡Œé‹ç®—çš„åŠŸèƒ½æ“ä½œã€‚

é€™ç¨®æ“ä½œé€šå¸¸è¢«èªç‚ºæ˜¯ä¸€å€‹éç·šæ€§å‡½æ•¸ï¼Œå®ƒä½¿å¾—ç¥ç¶“å…ƒåªæœ‰åœ¨è¼¸å…¥å€¼è¶…éæŸå€‹é–¾å€¼æ™‚æ‰æœƒç”Ÿæˆè¼¸å‡ºã€‚å› æ­¤ï¼Œé€šéé¡æ¯”ï¼Œç¥ç¶“ç¶²çµ¡å°è¼¸å…¥å€¼çš„åŠ æ¬Šå’Œæ‡‰ç”¨äº†ä¸€å€‹éç·šæ€§å‡½æ•¸ã€‚åœ¨ç¬¬ III-A1 ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡æ¢è¨é€™äº›éç·šæ€§å‡½æ•¸çš„å…·é«”å½¢å¼ã€‚

![alt text](image.png)

åœ–a è¼¸å…¥å±¤çš„ç¥ç¶“å…ƒæ¥æ”¶ä¸€äº›å€¼ï¼Œä¸¦å°‡é€™äº›å€¼å‚³éåˆ°ç¶²çµ¡çš„ä¸­é–“å±¤ç¥ç¶“å…ƒï¼Œè©²ä¸­é–“å±¤ä¹Ÿç¶“å¸¸è¢«ç¨±ç‚ºã€Œéš±è—å±¤ã€ã€‚ä¾†è‡ªä¸€å€‹æˆ–å¤šå€‹éš±è—å±¤çš„åŠ æ¬Šå’Œæœ€çµ‚æœƒå‚³éåˆ°è¼¸å‡ºå±¤ï¼Œè¼¸å‡ºå±¤å‘ç”¨æˆ¶å‘ˆç¾ç¶²çµ¡çš„æœ€çµ‚è¼¸å‡ºã€‚ ç¥ç¶“å…ƒçš„è¼¸å‡ºé€šå¸¸è¢«ç¨±ç‚ºæ¿€æ´»ï¼ˆactivationsï¼‰ï¼Œè€Œçªè§¸å‰‡é€šå¸¸è¢«ç¨±ç‚ºæ¬Šé‡ï¼ˆweightsï¼‰è·Ÿå¤§è…¦çš„é‹ä½œä¸€æ¨£

åœ–b ç¥ç¶“ç¶²çµ¡çš„é ˜åŸŸä¸­ï¼Œæœ‰ä¸€å€‹ç¨±ç‚ºæ·±åº¦å­¸ç¿’çš„åˆ†æ”¯ï¼Œå…¶ç‰¹é»æ˜¯ç¥ç¶“ç¶²çµ¡åŒ…å«å¤šæ–¼ä¸‰å±¤çš„çµæ§‹ï¼Œå³è¶…éä¸€å€‹éš±è—å±¤ã€‚
```

C. æ¨ç†èˆ‡è¨“ç·´

```txt
ç”±æ–¼æ·±åº¦ç¥ç¶“ç¶²çµ¡ (DNNs) æ˜¯æ©Ÿå™¨å­¸ç¿’ç®—æ³•çš„ä¸€ç¨®å¯¦ä¾‹ï¼Œå…¶åŸºæœ¬ç¨‹åºåœ¨å­¸ç¿’åŸ·è¡ŒæŒ‡å®šä»»å‹™çš„éç¨‹ä¸­ä¸¦ä¸æœƒæ”¹è®Šã€‚å°æ–¼ DNNs è€Œè¨€ï¼Œé€™ç¨®å­¸ç¿’éç¨‹æ¶‰åŠç¢ºå®šç¶²çµ¡ä¸­çš„æ¬Šé‡ï¼ˆå’Œåç½®ï¼‰çš„å€¼ï¼Œé€™è¢«ç¨±ç‚ºã€Œè¨“ç·´ç¶²çµ¡ã€ã€‚

ä¸€æ—¦è¨“ç·´å®Œæˆï¼Œç¨‹åºå°±å¯ä»¥ä½¿ç”¨åœ¨è¨“ç·´éç¨‹ä¸­ç¢ºå®šçš„æ¬Šé‡ä¾†è¨ˆç®—ç¶²çµ¡çš„è¼¸å‡ºä¸¦åŸ·è¡Œå…¶ä»»å‹™ã€‚ä½¿ç”¨é€™äº›æ¬Šé‡é‹è¡Œç¨‹åºçš„éç¨‹è¢«ç¨±ç‚ºã€Œæ¨ç†ã€ï¼ˆinferenceï¼‰ã€‚

æœ¬æ–‡å°‡é‡é»æ”¾åœ¨ DNN æ¨ç†çš„é«˜æ•ˆè™•ç†ä¸Šï¼Œè€Œéè¨“ç·´ï¼Œå› ç‚º DNN æ¨ç†é€šå¸¸æ˜¯åœ¨åµŒå…¥å¼è¨­å‚™ï¼ˆè€Œéé›²ç«¯ï¼‰ä¸ŠåŸ·è¡Œï¼Œé€™äº›è¨­å‚™çš„è³‡æºæœ‰é™ã€‚
```

æ¨ç†èˆ‡è¨“ç·´çš„å€åˆ¥

1.æ¨ç†

- å·²è¨“ç·´å¥½çš„ DNN ä½¿ç”¨å›ºå®šçš„æ¬Šé‡å’Œåç½®é€²è¡Œè¨ˆç®—ï¼Œä¾†åŸ·è¡Œç‰¹å®šä»»å‹™ï¼Œä¾‹å¦‚åœ–åƒåˆ†é¡ã€‚
- æ¨ç†çš„ç›®çš„æ˜¯æ ¹æ“šè¼¸å…¥è³‡æ–™ç”¢ç”Ÿè¼¸å‡ºçµæœï¼Œé€šå¸¸æ‡‰ç”¨æ–¼è³‡æºæœ‰é™çš„è¨­å‚™ï¼ˆå¦‚åµŒå…¥å¼è¨­å‚™ï¼‰ã€‚
- ç²¾åº¦éœ€æ±‚è¼ƒä½ï¼Œå¯æ¡ç”¨ä¸€äº›é™ä½ç²¾åº¦çš„æŠ€è¡“ä»¥æé«˜æ•ˆç‡ã€‚

2.è¨“ç·´

- ç›®çš„æ˜¯é€éèª¿æ•´æ¬Šé‡èˆ‡åç½®ï¼Œä½¿ç¶²çµ¡çš„è¼¸å‡ºæ›´æ¥è¿‘ç›®æ¨™çµæœï¼Œæœ€å°åŒ–æå¤±å‡½æ•¸ ğ¿ã€‚
- åˆ©ç”¨å„ªåŒ–æ–¹æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰ä¾†é€æ­¥æ›´æ–°æ¬Šé‡ã€‚
- è¨“ç·´éç¨‹éœ€è¦å¤§é‡è¨ˆç®—è³‡æºèˆ‡è¼ƒé«˜çš„æ•¸å€¼ç²¾åº¦ï¼Œä¸¦ä¸”éœ€è¦å­˜å„²ä¸­é–“è¼¸å‡ºä»¥é€²è¡Œåå‘å‚³æ’­ï¼ˆBackpropagationï¼‰ã€‚

è¨“ç·´çš„æŠ€å·§èˆ‡é¡å‹

1.è¨“ç·´æŠ€è¡“

- ä½¿ç”¨ã€Œæ‰¹é‡ã€æå¤±æ›´æ–°æ¬Šé‡ï¼šç´¯ç©å¤šçµ„æ•¸æ“šå¾Œä¸€æ¬¡æ›´æ–°ï¼Œå¯åŠ é€Ÿå’Œç©©å®šè¨“ç·´éç¨‹ã€‚
- åå‘å‚³æ’­ï¼šé€šééˆå¼æ³•å‰‡è¨ˆç®—æ¯å€‹æ¬Šé‡å°æå¤±çš„åå°æ•¸ï¼Œé€²è¡Œé«˜æ•ˆæ¢¯åº¦è¨ˆç®—ã€‚

2.å­¸ç¿’é¡å‹

- ç›£ç£å­¸ç¿’ï¼šä½¿ç”¨å¸¶æœ‰æ¨™ç±¤çš„æ•¸æ“šé€²è¡Œè¨“ç·´ï¼ˆæœ€å¸¸è¦‹ï¼‰ã€‚
- ç„¡ç›£ç£å­¸ç¿’ï¼šç”¨æœªæ¨™ç±¤æ•¸æ“šæ‰¾å‡ºçµæ§‹æˆ–ç¾¤é›†ã€‚
- åŠç›£ç£å­¸ç¿’ï¼šçµåˆå°‘é‡æ¨™ç±¤æ•¸æ“šå’Œå¤§é‡æœªæ¨™ç±¤æ•¸æ“šã€‚
- å¼·åŒ–å­¸ç¿’ï¼šåŸºæ–¼ç’°å¢ƒåé¥‹é€²è¡Œå‹•ä½œé¸æ“‡ï¼Œç›®æ¨™æ˜¯æœ€å¤§åŒ–é•·æœŸå›å ±ã€‚

3.å¾®èª¿ï¼ˆFine-Tuningï¼‰

- ä½¿ç”¨å…ˆå‰è¨“ç·´å¥½çš„æ¨¡å‹æ¬Šé‡ä½œç‚ºèµ·é»ï¼Œå†é‡å°æ–°æ•¸æ“šæˆ–ç´„æŸæ¢ä»¶é€²è¡Œèª¿æ•´ã€‚
- å„ªé»ï¼šåŠ å¿«è¨“ç·´é€Ÿåº¦ï¼Œä¸¦å¯èƒ½æå‡æº–ç¢ºæ€§ï¼ˆå¦‚é·ç§»å­¸ç¿’ï¼‰ã€‚

4.è¨“ç·´èˆ‡æ¨ç†çš„è³‡æºéœ€æ±‚å·®ç•°

- è¨“ç·´ éœ€è¦æ›´å¤šè¨ˆç®—è³‡æºèˆ‡å­˜å„²ï¼Œå› ç‚ºå¿…é ˆä¿å­˜ä¸­é–“çµæœä¸¦é€²è¡Œç²¾ç¢ºçš„æ¢¯åº¦è¨ˆç®—ã€‚
- æ¨ç† è³‡æºéœ€æ±‚è¼ƒä½ï¼Œç‰¹åˆ¥æ˜¯åœ¨æ‡‰ç”¨æ–¼åµŒå…¥å¼è¨­å‚™æ™‚ï¼Œå¯ä»¥æ¡ç”¨é™ä½ç²¾åº¦çš„æŠ€è¡“ä»¥æé«˜é‹ç®—æ•ˆç‡ã€‚

æ·±åº¦å­¸ç¿’æˆåŠŸçš„ä¸‰å¤§å› ç´ 
1.å¤§é‡çš„è¨“ç·´æ•¸æ“š

- å¼·å¤§çš„è¡¨ç¤ºå­¸ç¿’éœ€è¦å¤§é‡æ•¸æ“šæ”¯æŒã€‚
- ä¾‹å­ï¼š
  - Facebook æ¯æ—¥æ¥æ”¶ 10 å„„å¼µåœ–ç‰‡ã€‚
  -Walmart æ¯å°æ™‚å‰µå»º 2.5 PB çš„å®¢æˆ¶æ•¸æ“šã€‚
  -YouTube æ¯åˆ†é˜ä¸Šå‚³ 300 å°æ™‚å½±ç‰‡ã€‚
- å·¨é‡æ•¸æ“šæˆç‚ºè¨“ç·´æ¼”ç®—æ³•çš„åŸºçŸ³ã€‚

2.å¼·å¤§çš„è¨ˆç®—èƒ½åŠ›

- åŠå°é«”æŠ€è¡“èˆ‡è¨ˆç®—æ¶æ§‹çš„é€²æ­¥æå‡äº†é‹ç®—æ•ˆèƒ½ã€‚
- DNN ä¸­çš„å¤§é‡åŠ æ¬Šæ±‚å’Œé‹ç®—å¦‚ä»Šèƒ½åœ¨åˆç†æ™‚é–“å…§å®Œæˆï¼Œæˆç‚ºè¨“ç·´èˆ‡æ¨ç†çš„åŸºç¤ã€‚

3.æ¼”ç®—æ³•èˆ‡å·¥å…·çš„é€²åŒ–

- DNN çš„æˆåŠŸæ¿€ç™¼äº†æ›´å¤šæ¼”ç®—æ³•çš„ç™¼å±•ã€‚
- é–‹æºæ¡†æ¶ï¼ˆå¦‚ TensorFlowã€PyTorch ç­‰ï¼‰çš„ç™¼å±•ä½¿ç ”ç©¶è€…æ›´å®¹æ˜“æ¢ç´¢èˆ‡æ‡‰ç”¨ DNNã€‚
- æ–°æŠ€è¡“ä¸åƒ…æå‡äº†æ‡‰ç”¨çš„æº–ç¢ºæ€§ï¼Œä¹Ÿæ“´å±•äº† DNN çš„é©ç”¨ç¯„åœã€‚
